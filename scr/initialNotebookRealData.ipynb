{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Jupyter notebook for data mining project with real data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all libraries needed for the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas import DataFrame, read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # this is how I usually import pandas\n",
    "import sys  # only needed to determine Python version number\n",
    "from string import punctuation\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import StemmerI, RegexpStemmer, LancasterStemmer, ISRIStemmer, PorterStemmer, SnowballStemmer, RSLPStemmer\n",
    "from nltk import word_tokenize\n",
    "# nltk.download(u'stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from operator import add\n",
    "import random\n",
    "from numpy import array\n",
    "\n",
    "# import enchant\n",
    "# import hunspell\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# import my functions\n",
    "from myFunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Location = r'../twitter_data/train2017.tsv'\n",
    "df = pd.read_csv(Location, sep='\\t', names=['ID_1', 'ID_2', 'Label', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only a part of csv\n",
    "# df = df[:10000]\n",
    "# Preprocess the traindata\n",
    "processed_list = preprocess(df)\n",
    "\n",
    "# print data\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLocation = r'../twitter_data/test2017.tsv'\n",
    "testDf = pd.read_csv(testLocation, sep='\\t', names=['ID_1', 'ID_2', 'Label', 'Text'])\n",
    "\n",
    "# use only a part of csv\n",
    "testDf = testDf[:10000]\n",
    "\n",
    "# Preprocess the testData\n",
    "processed_list_test = preprocess(testDf)\n",
    "\n",
    "# print data\n",
    "# testDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the correct results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsLocation = r'../twitter_data/SemEval2017_task4_subtaskA_test_english_gold.txt'\n",
    "testResults = pd.read_csv(resultsLocation, sep='\\t', names=['ID', 'Label'])\n",
    "\n",
    "# use only a part of csv\n",
    "testResults = testResults[:10000]\n",
    "\n",
    "# print data\n",
    "# testResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build label encoder for categories\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape is:\n",
      "(10000,)\n",
      "y_test.shape is:\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# Transform categories into numbers\n",
    "y = le.transform(df[\"Label\"])\n",
    "print(\"y.shape is:\")\n",
    "print(y.shape)\n",
    "y_test = le.transform(testResults[\"Label\"])\n",
    "print(\"y_test.shape is:\")\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get processed content for list\n",
    "processed_content = [item[1] for item in processed_list]\n",
    "processed_content_test = [item[1] for item in processed_list_test]\n",
    "# processed_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization using count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize Content\n",
    "# Choose one of the below\n",
    "\n",
    "# CountVectorizer (BOW)\n",
    "\n",
    "# count_vectorizer = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "# X = count_vectorizer.fit_transform(processed_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization using TfId vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfIdfVectorizer\n",
    "# train and test vectors should have the same number of features\n",
    "\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(processed_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization using word embeddings\n",
    "\n",
    "Train the word embeddings model and save it to a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embeddings\n",
    "tokenize = lambda x: x.split()\n",
    "tokenized_tweet = [tokenize(x) for x in processed_content] # tokenizing\n",
    "# print(tokenized_tweet)\n",
    "vec_size = 200\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            size=vec_size,  # desired no. of features/independent variables\n",
    "            window=7,  # context window size\n",
    "            min_count=5,\n",
    "            sg=1,  # 1 for skip-gram model\n",
    "            hs=0,\n",
    "            negative=10,  # for negative sampling\n",
    "            workers=2,  # no.of cores\n",
    "            seed=34)\n",
    "\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(processed_content), epochs=20)\n",
    "\n",
    "model_w2v.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained word embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "# tsne_plot(model_w2v)\n",
    "# print(model_w2v.wv.vocab)\n",
    "# print(model_w2v[\"obama\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the vectors for the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "X.shape is:\n",
      "(10000, 200)\n"
     ]
    }
   ],
   "source": [
    "# tokenize = lambda x: x.split()\n",
    "processed_content_vec = []\n",
    "for tweet in processed_content:\n",
    "    tweet_len = len(tweet)\n",
    "    if tweet_len == 0:\n",
    "        tweet_vec = sample_floats(-5.0, 5.0, vec_size)\n",
    "        processed_content_vec.append(tweet_vec)\n",
    "        continue\n",
    "    tokenized_tweet = tokenize(tweet)\n",
    "    if tokenized_tweet[0] in model_w2v.wv.vocab:\n",
    "        tweet_vec = model_w2v.wv[tokenized_tweet[0]]\n",
    "    else:\n",
    "        tweet_vec = sample_floats(-5.0, 5.0, vec_size)\n",
    "    for token in tokenized_tweet[1:]:\n",
    "        if token in model_w2v.wv.vocab:\n",
    "            tweet_vec = list(map(add, tweet_vec, model_w2v.wv[token]))\n",
    "        else:\n",
    "            tweet_vec = list(map(add, tweet_vec, sample_floats(-5.0, 5.0, vec_size)))\n",
    "    final_tweet_vec = [i/tweet_len for i in tweet_vec]\n",
    "    processed_content_vec.append(final_tweet_vec)\n",
    "\n",
    "X = array(processed_content_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the vectors for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "X_test.shape is:\n",
      "(10000, 200)\n"
     ]
    }
   ],
   "source": [
    "processed_content_test_vec = []\n",
    "print(len(processed_content_test))\n",
    "for tweet in processed_content_test:\n",
    "    tweet_len = len(tweet)\n",
    "    if tweet_len == 0:\n",
    "        tweet_vec = sample_floats(-5.0, 5.0, vec_size)\n",
    "        processed_content_test_vec.append(tweet_vec)\n",
    "        continue\n",
    "    tokenized_tweet = tokenize(tweet)\n",
    "    if tokenized_tweet[0] in model_w2v.wv.vocab:\n",
    "        tweet_vec = model_w2v.wv[tokenized_tweet[0]]\n",
    "    else:\n",
    "        tweet_vec = sample_floats(-5.0, 5.0, vec_size)\n",
    "    for token in tokenized_tweet[1:]:\n",
    "        if token in model_w2v.wv.vocab:\n",
    "            tweet_vec = list(map(add, tweet_vec, model_w2v.wv[token]))\n",
    "        else:\n",
    "            tweet_vec = list(map(add, tweet_vec, sample_floats(-5.0, 5.0, vec_size)))\n",
    "    final_tweet_vec = [i/tweet_len for i in tweet_vec]\n",
    "    processed_content_test_vec.append(final_tweet_vec)\n",
    "\n",
    "X_test = array(processed_content_test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See theshapes of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape is:\n",
      "(10000, 200)\n",
      "X_test.shape is:\n",
      "(10000, 200)\n"
     ]
    }
   ],
   "source": [
    "# X = vectorizer.fit_transform(processed_content)\n",
    "print(\"X.shape is:\")\n",
    "print(X.shape)\n",
    "\n",
    "# X_test = vectorizer.transform(processed_content_test)\n",
    "print(\"X_test.shape is:\")\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred.shape is:\n",
      "(10000,)\n",
      "\n",
      "classification report for these predictions is:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00      3220\n",
      "     neutral       0.50      0.66      0.57      4812\n",
      "    positive       0.25      0.47      0.32      1968\n",
      "\n",
      "   micro avg       0.41      0.41      0.41     10000\n",
      "   macro avg       0.25      0.37      0.30     10000\n",
      "weighted avg       0.29      0.41      0.34     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spithas/anaconda3/envs/DM_env/lib/python2.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# fit train set\n",
    "clf.fit(X, y)\n",
    "\n",
    "# predict test set (here is the same as the train set)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"y_pred.shape is:\")\n",
    "print(y_pred.shape)\n",
    "\n",
    "# print('\\npredictions of test set (which is the same as the train set) are:')\n",
    "# print(y_pred)\n",
    "\n",
    "# Transform predictions to text\n",
    "predicted_categories = le.inverse_transform(y_pred)\n",
    "# print('\\npredictions of test set in text form are:')\n",
    "# print(predicted_categories)\n",
    "\n",
    "\n",
    "# Classification_report\n",
    "print('\\nclassification report for these predictions is:')\n",
    "print(classification_report(y_test, y_pred, target_names=list(le.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification using KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "classification report for these predictions is:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.34      0.17      0.23      3220\n",
      "     neutral       0.50      0.53      0.51      4812\n",
      "    positive       0.24      0.40      0.30      1968\n",
      "\n",
      "   micro avg       0.39      0.39      0.39     10000\n",
      "   macro avg       0.36      0.37      0.35     10000\n",
      "weighted avg       0.40      0.39      0.38     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use KNNClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# fit train set\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Predict test set (here is the same as the train set)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# print('\\npredictions of test set (which is the same as the train set) are:')\n",
    "# print(y_pred)\n",
    "\n",
    "# Transform predictions to text\n",
    "predicted_categories = le.inverse_transform(y_pred)\n",
    "# print('\\npredictions of test set in text form are:')\n",
    "# print(predicted_categories)\n",
    "\n",
    "# Classification_report\n",
    "print('\\nclassification report for these predictions is:')\n",
    "print(classification_report(y_test, y_pred, target_names=list(le.classes_)))\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "region,endregion",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "dm_kernel",
   "language": "python",
   "name": "dm_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
